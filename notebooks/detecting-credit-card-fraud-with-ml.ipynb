{"cells":[{"metadata":{"_cell_guid":"e42ca4de-111e-4272-a27f-1af3ec16b0e6","_uuid":"a3cf743f95457f4d45b869cd8643fd0b629e0532"},"cell_type":"markdown","source":"# Detecting Credit Card Fraud with Machine Learning"},{"metadata":{"_cell_guid":"2298d680-d5bb-41b8-8ade-1a3ed2efbdfd","_uuid":"ea03b308b0a32bf9278cb9ea56d63a2089952308"},"cell_type":"markdown","source":"Let's see if we can predict whether or not a given transacation is fraudulent using the given dataset."},{"metadata":{"_cell_guid":"bd6424c8-8e0c-4fe3-ac87-f46a7a835458","_uuid":"0d2d4e2c7ef85595e694a0ef24fcb31b0f844c9b"},"cell_type":"markdown","source":"### Loading and Observing the Data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\nprint(os.listdir(\"../input\"))","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"Let's first load the data into a pandas dataframe and take a peek at its contents."},{"metadata":{"_cell_guid":"9d86ee8c-ccc2-491a-9687-793654d57714","collapsed":true,"_uuid":"f6636c1dad5a8d11cc736b4bd7e34af7c1a3f1c1","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"e5c31c29-8c96-41a5-9a8b-ad02581eb440","_uuid":"8190f409afcbfef31d3af56ba06501d6d3da0b88","trusted":true,"collapsed":true},"cell_type":"code","source":"data.head()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"8dceef40-a65e-4fea-a609-a35e3ff65a61","_uuid":"90dd0b749a9f7ccbd94ed0a1940cb3057b2d9b9b","trusted":true,"collapsed":true},"cell_type":"code","source":"data.describe()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79d1d044-b24c-4714-bb83-73ab44bd6935","_uuid":"e9cf42dc880efe0d7358d73835c2b334395976cc"},"cell_type":"markdown","source":"There is apparently a huge class imbalance in this dataset. We can confirm this by plotting a seaborn countplot of the different class labels."},{"metadata":{"_cell_guid":"76c48348-35a4-4d8f-aed2-5d3ad8cf8c4d","_uuid":"1754462d4ac4687c3031533283064edc0936add8","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.countplot(data[\"Class\"])","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"a1902e3d-fef4-44f0-b052-bc362b762b1f","_uuid":"a5eb06883de1752bdc420a62d9a8b64401ba344b"},"cell_type":"markdown","source":"As we can see from the above plot, fraudulent activities make up a very small fraction of this dataset. Let's further check to see if there are any null values in the data."},{"metadata":{"_cell_guid":"7143ba4d-b836-47db-9838-8e326b9d88d7","_uuid":"0caebc6c43ba2fa73ec6b34e5079296dd19ee8fd","trusted":true,"collapsed":true},"cell_type":"code","source":"data.isnull().any().describe()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"e3965cdf-0d82-45e2-82c9-455a01ee09f6","_uuid":"e0c98c84fbfdf0fde8b29c6278c025fe02879bb3"},"cell_type":"markdown","source":"As we can see, the dataset is not missing any values."},{"metadata":{"_cell_guid":"dc5fc052-8cf7-4b49-b04d-b11876033be8","_uuid":"9c7e665fc72a9e9c63f42f2ec863f479c1d69e32"},"cell_type":"markdown","source":"### Preparing a Train and Test Set"},{"metadata":{"_cell_guid":"f4eb5053-37fe-4650-ad27-ea8a65e37faa","_uuid":"32faec95f57fcbbb8b05ada2034c367c70f101e2"},"cell_type":"markdown","source":"Let's partition our dataset into a train and test set, where 90% of the data will be a part of the training set, 5% will be allocated for validation, and 5% will be used for testing."},{"metadata":{"_cell_guid":"beec7e9e-a893-4658-929b-7ffa7abd989d","collapsed":true,"_uuid":"cce2cb78a20aacf9c408d6d819f13ad1f9d2f337","trusted":true},"cell_type":"code","source":"limit = int(0.9*len(data))\ntrain = data.loc[:limit]\ndev_test = data.loc[limit:]\ndev_test.reset_index(drop=True, inplace=True)\ndev_test_limit = int(0.5*len(dev_test))\ndev = dev_test.loc[:dev_test_limit]\ntest = dev_test.loc[dev_test_limit:]","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"9a11fff9-91b6-4402-bb51-c9b15d4e1acc","_uuid":"56d704338f75039e4fd6f382f513765542a8cd3f"},"cell_type":"markdown","source":"Let's check to see that the validation and test set include a fair amount of fraudulent activites before going any further."},{"metadata":{"_cell_guid":"47ae2345-b6bf-4053-b339-b3639cc7fc40","_uuid":"3bd9317a824f9726cea8081af5ee3978eeae2aa9","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Number of fraudulent transactions in the dev set: {}\".format(dev[\"Class\"].value_counts()[1]))\nprint(\"Number of fraudulent transactions in the test set: {}\".format(test[\"Class\"].value_counts()[1]))","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"fb9e71cc-2a00-4671-a0c2-d4b6db63667b","_uuid":"12616df9241a535d2df7b54b9b6b5749c611aed0"},"cell_type":"markdown","source":"Now we can focus on developing a model to accurately detect fraudulent activity. Due to the huge class imbalance in our dataset, a model that simply identifies all transactions as not being fraudulent would score high accuracy. There also would not be many fraudulent samples for the model to learn from to be able to accurately identify what a fraulent transaction is. Therefore, we should find a way to balance out the number of positive and negatives instances in our training set. This can be done by either oversampling the positive instances, or negatively sampling the negative instances. Negatively sampling the negative instances would involving reducing the number of not-fraudulent transactions until the ratio between positive and negative instances was approximately 1-to-1. Since we don't have that many data samples, I fear doing so would severely limit our model's performance, since it would have much less data to train on. We shall therefore oversample the positive instances in our dataset."},{"metadata":{"_cell_guid":"927bdb98-7cfa-408f-acea-de484c7ce379","_uuid":"eabbc7e88b808e554fcf439a4be4084ec66a1040"},"cell_type":"markdown","source":"To oversample the positive instances in our training set, we will add copies of them to it, but with their feature values slightly tweaked. This manipulation of the data is to allow for there to be more positive instances in the dataset, with this manipulation only being slight so has to not change the data too much as to end up teaching our model false information. This tweaking will be done by multiplying each positive sample copy's feature values by a number between the uniform distribution of 0.9 and 1.1."},{"metadata":{"_cell_guid":"2f3d57ce-fa5d-454f-a9e2-0382b817856d","collapsed":true,"_uuid":"816220701f77259e8e149058d5c8bcce0520e113","trusted":true},"cell_type":"code","source":"train_positive = train[train[\"Class\"] == 1]\ntrain_positive = pd.concat([train_positive] * int(len(train) / len(train_positive)), ignore_index=True)\nnoise = np.random.uniform(0.9, 1.1, train_positive.shape)\ntrain_positive = train_positive.multiply(noise)\ntrain_positive[\"Class\"] = 1\ntrain_extended = train.append(train_positive, ignore_index=True)\ntrain_shuffled = train_extended.sample(frac=1, random_state=0).reset_index(drop=True)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"f912cf1a-cb85-4002-bdeb-da2489994210","_uuid":"a18e67568c464bdde42c71500d44e414e82cf36f"},"cell_type":"markdown","source":"The ratio of positive to negative instances in our training set should now be much more balanced."},{"metadata":{"_cell_guid":"14a9c02f-1913-455f-9bce-52ad5e56aa69","_uuid":"6f7ab05ccaeb41e3bc34d4955a05fcb87d0574b1","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.countplot(train_shuffled[\"Class\"])","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"71a00d5f-06b0-4de0-a2a8-d04e499f7739","_uuid":"12e018b8db1b3df362b95e54e31572a624a8ff2b"},"cell_type":"markdown","source":"With the class imbalance in our training set dealt with we can now separate our training, validation, and test sets into their respective features and responses."},{"metadata":{"_cell_guid":"3a06f968-8098-4d8d-8de6-ba3a3eefeffc","collapsed":true,"_uuid":"e26f2bbda5ebcb35fad38df0da41ffd7fe4b28ca","trusted":true},"cell_type":"code","source":"X_train = train_shuffled.drop(labels=[\"Class\"], axis=1)\nY_train = train_shuffled[\"Class\"]\nX_dev = dev.drop(labels=[\"Class\"], axis=1)\nY_dev = dev[\"Class\"]\nX_test = test.drop(labels=[\"Class\"], axis=1)\nY_test = test[\"Class\"]","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"5c8c3a90-9fee-4444-90f4-f8b271959aa0","_uuid":"58fea69032bb95a5d1fac88d98cc96da7c83579b"},"cell_type":"markdown","source":"### Training and Validation a Fraudulent Activity Detector"},{"metadata":{"_cell_guid":"658727b2-c7f1-42c4-9e21-d74e503ade4f","collapsed":true,"_uuid":"2140690e51f3634a5c35c82ce442b3e222cdfd71"},"cell_type":"markdown","source":"Let's begin by training a simple logistic regression model on our training set and see how well that does on our validation set."},{"metadata":{"_cell_guid":"c2251be1-8a97-4324-9ffb-ccb34bd25a4d","_uuid":"15e9d5163a63d4566ff63570ffcfcc8743c2ec4e","trusted":true,"collapsed":true},"cell_type":"code","source":"lr_model = LogisticRegression(random_state=0).fit(X_train, Y_train)\nprint(\"Train Accuracy:\", lr_model.score(X_train, Y_train))\nprint(\"Dev Accuracy:\", lr_model.score(X_dev, Y_dev))","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"0d42ddd9-89f8-4265-ad54-2296d075faec","_uuid":"c9a5de313016cca0c74e899ffb71ad7f59c051c4"},"cell_type":"markdown","source":"Though achieving over 98% accuracy on our validation data is thrilling, we cannot forget about the huge class imbalance still present in the validation set. A model that simply outputted that there were no transactions would achieve high accuracy as well. The dataset info recommends using the AUPRC as an evaluation metric. We will use the average precision score instead, which is an evaluation metric available in scikit-learn that is sometimes used as an alternative to AUPRC,"},{"metadata":{"_cell_guid":"9fe3b764-684f-4112-aa1e-62c138172547","_uuid":"58ba0e4597b527f5f7422ccfb9cfc51be81abe17"},"cell_type":"markdown","source":"Let's take a look and see what the above model achieved on the training and validation set's average precision score."},{"metadata":{"_cell_guid":"7d0d4ac8-4c75-4233-85d4-87bf9ac23670","_uuid":"58979d66e795d61af15489e86fcd0e2b724650ae","trusted":true,"collapsed":true},"cell_type":"code","source":"lr_predict_train = lr_model.predict(X_train)\nlr_predict_dev = lr_model.predict(X_dev)\nprint(average_precision_score(Y_train, lr_predict_train))\nprint(average_precision_score(Y_dev, lr_predict_dev))","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"92676c82-d489-4bef-a2d0-3c68bb49ae3d","_uuid":"08550ad991c6d301ecbc1d41a030749737e60d34"},"cell_type":"markdown","source":"As we can see, this model didn't perform nearly as well as we initially thought. Viewing a confusion matrix of the predictions made by the model, we can gather a better idea of what type of errors it is making."},{"metadata":{"_cell_guid":"cf76e33c-2720-4b9a-bb51-2fc518e0b448","_uuid":"522c1e26884fb465fcce0e21fca0dc268bfda342","trusted":true,"collapsed":true},"cell_type":"code","source":"lr_confusion_dev = pd.DataFrame(confusion_matrix(Y_dev, lr_predict_dev))\nlr_confusion_dev.columns = [\"Predicted Negative\", \"Predicted Positive\"]\nlr_confusion_dev.index = [\"Actual Negative\", \"Actual Positive\"]\nsns.heatmap(lr_confusion_dev, annot=True)\nplt.yticks(rotation=0)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"d4724810-1c81-4548-93a5-fcb2f5838a6d","_uuid":"a345b2351e60079c43bf152ace84e26faa8fcbee"},"cell_type":"markdown","source":"Viewing the confusion matrix, we can see that the logistic regression model identified all but one of the fraudulent transactions as such. However, we are also misclassifying 28 non-fraudulent activities in the validation set as fraudulent. Though I would much rather our model make false positive errors rather than false negatives, let's see if we can improve our results by using several other machine learning models."},{"metadata":{"collapsed":true,"_uuid":"e74c550752fa3f0e08a9f892b8d32c9e78b6e54a","trusted":false},"cell_type":"markdown","source":"### To be continued..."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e1437d7e341611b8a8d894bf3b3b07eae3aad1ed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}