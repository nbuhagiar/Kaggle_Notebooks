{"cells":[{"metadata":{"_uuid":"89a457c48ec4bad15486c0f4f5ef0079a441b09d"},"cell_type":"markdown","source":"# LR Breast Cancer Diagnoser"},{"metadata":{"_uuid":"7bb0e016fdd965397abeb068608617d4df6e08bf"},"cell_type":"markdown","source":"Using a dataset consisting of the characteristics of digitized images of the cell nuceli in breast masses, let's see if we can use machine learning to build a model that can diagnose a breast mass as benign or malignant given this information."},{"metadata":{"_uuid":"a27e8c725fe2882b9afc27bbd75d911c377f5e59"},"cell_type":"markdown","source":"## Data Exploration and Preprocessing"},{"metadata":{"_uuid":"db9c28d5f2fea0b7e7dc382e42935252effafd86"},"cell_type":"markdown","source":"Let's begin by loading in the necessary packages and checking to see what file(s) we have in our dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import fbeta_score,confusion_matrix\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Let's read our data file into a pandas dataframe and take a peek at its contents."},{"metadata":{"trusted":true,"_uuid":"088a425737f419bb99def68cd6234880239c3a14"},"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\", index_col=\"id\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d941d95a1fb2b27c8dea422eb3a041a4ceb2264"},"cell_type":"markdown","source":"As we can see, we have data pertaining to the medical diagnosis of breast masses, with many different features pertaining to the diagnosis such as the average radius or texture of the mass cell nuclei image. As articulated from the dataset description, this is a binary classification problem, with these masses being identified as either benign (represented by the label 'B') or malignant (represented by the label 'M').\n\nLet's gather some statistical information about our dataset before proceeding."},{"metadata":{"trusted":true,"_uuid":"e177513266fba6c9984957a48af7a197b3aeb5c8"},"cell_type":"code","source":"data.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9097b8762d6dd46948a87df977d5ddba5d982cec"},"cell_type":"markdown","source":"Viewing the above dataset description, we clearly have an unnecessary column in our dataset containing no data named 'Unnamed: 32'. We can confidently drop this column from our dataset."},{"metadata":{"trusted":true,"_uuid":"95b51c449b49d6f870a94fbd26b63e66d9d9b692"},"cell_type":"code","source":"data.drop(\"Unnamed: 32\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80ccbac471b870d284765dd581a4728d42024641"},"cell_type":"markdown","source":"Viewing the above dataset description, we're also thankfully missing no data points, which is helpful since there are only 569 data instances in our dataset. All of the features in our dataset seem to have differing feasible ranges for their values, so this data will definitely have to be standardized prior to training a machine learning model on it as well. Let's view the class distribution in our dataset."},{"metadata":{"trusted":true,"_uuid":"446362960b11cb262b15ec360acf265b03be79ea"},"cell_type":"code","source":"ax = sns.countplot(data.diagnosis)\nax.set_xticklabels([\"Malignant\", \"Benign\"])\nax.set_xlabel(\"Diagnosis\")\nax.set_ylabel(\"Number of Data Samples\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"379bdd5774c351151e783039c581c46e49dca710"},"cell_type":"markdown","source":"As we can see from the above plot, though we have definitely more 'benign' than 'malignant' samples in our dataset, this discrepancy doesn't seem to be large enough to be of much concern.\n\nLet's now separate our dataset into a train and test set, with 80% of the data samples being placed in to the training set, and 20% being placed in the test set, ensuring that the class distribution remains relatively the same in both datasets."},{"metadata":{"trusted":true,"_uuid":"49f73ead52777531cd7f2f44d829521db931e5ae"},"cell_type":"code","source":"X = data.drop(\"diagnosis\", axis=1)\ny = data.diagnosis\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=0,\n                                                    stratify=y)\nX_train = X_train.copy()\ny_train = y_train.copy()\nX_test = X_test.copy()\ny_test = y_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c826e4079182c5e4157ebe933f0373113d5392f"},"cell_type":"markdown","source":"Let's also standardize the values in both the train and test sets using a standardizer fit to the training data."},{"metadata":{"trusted":true,"_uuid":"60b7d934b0e51cf11ce5fcf460183c11f9722207"},"cell_type":"code","source":"scaler = StandardScaler()\nX_train[X_train.columns] = scaler.fit_transform(X_train)\nX_test[X_test.columns] = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"506c9e0b8fe36f6e1f36085a8cd6f52d9b31ebf4"},"cell_type":"markdown","source":"With the data satisfactorily preprocessed, let's proceed to build a supervised machine learning model to diagnose breast cancer."},{"metadata":{"_uuid":"f04e8c36563b677c53ddb92e5bb5b1fdcd20ee38"},"cell_type":"markdown","source":"## Diagnosing Breast Cancer with Machine Learning"},{"metadata":{"_uuid":"2831a2180aac62fa5f1fd12bd6add810ab85f936"},"cell_type":"markdown","source":"### Training a Model"},{"metadata":{"_uuid":"b583ae00152536ee2b7a3938c729464b4c9eaa31"},"cell_type":"markdown","source":"Let's train several different machine learning models to diagnose a breast mass as either benign or malignant. We will begin by training a logistic regression (LR) model for this task, using cross-validation to identify the best hyperparameters to use."},{"metadata":{"trusted":true,"_uuid":"96d9f8a9a900c411225309600c740d9f2f1851ca"},"cell_type":"code","source":"lr_param_grid = dict(class_weight=(None, \"balanced\"),\n                     penalty=(\"l1\", \"l2\"),\n                     C=np.logspace(-3, 3, 7))\nlr_cv = GridSearchCV(LogisticRegression(solver=\"liblinear\", random_state=0),\n                     lr_param_grid,\n                     cv=5,\n                     iid=False)\nbest_lr_params = lr_cv.fit(X_train, y_train).best_params_\nlr_model = LogisticRegression(penalty=best_lr_params[\"penalty\"],\n                              C=best_lr_params[\"C\"],\n                              class_weight=best_lr_params[\"class_weight\"],\n                              solver=\"liblinear\",\n                              random_state=0)\nlr_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6987b92ab7f94f2fc6263078aa72519bf9c54eb"},"cell_type":"markdown","source":"Support vector machines (SVMs) are known to perform well on small datasets. Let's perform cross-validation to find the best hyperparameters to use to train an SVM for this task as well, checking to see what is the best kernel to use for this task."},{"metadata":{"trusted":true,"_uuid":"fe35c8b9139bbcc220748e79c7c24b7b29cc5d78"},"cell_type":"code","source":"svm_param_grid = dict(class_weight=(None, \"balanced\"),\n                      C=np.logspace(-3, 3, 7),\n                      kernel=(\"linear\", \"poly\", \"rbf\", \"sigmoid\"),\n                      degree=(2, 3),\n                      gamma=(\"auto\", \"scale\"),\n                      shrinking=(True, False))\nsvm_cv = GridSearchCV(SVC(random_state=0),\n                      svm_param_grid,\n                      cv=5,\n                      iid=False)\nbest_svm_params = svm_cv.fit(X_train, y_train).best_params_\nprint(f\"A {best_svm_params['kernel']} SVM kernel should be used for this task.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8c817a0f51fbd4925822188a142d7a0972b0d83"},"cell_type":"markdown","source":"It seems that an SVM with a linear kernel performs the best in cross-validation. Since sci-kit learn has its own specific LinearSVC class optimized for an SVM with a linear kernel, let's perform further cross-validation to train a linear SVM to hopefully achieve superior results."},{"metadata":{"trusted":true,"_uuid":"c020135c885a25b96b7d724da070e2a42e1ea5d1"},"cell_type":"code","source":"lsvm_param_grid = dict(class_weight=(None, \"balanced\"),\n                       penalty=(\"l1\", \"l2\"),\n                       loss=(\"hinge\", \"squared_hinge\"),\n                       dual=(True, False),\n                       C=np.logspace(-3, 3, 7))\nlsvm_cv = GridSearchCV(LinearSVC(random_state=0),\n                       lsvm_param_grid,\n                       cv=5,\n                       iid=False,\n                       error_score=np.nan)\nbest_lsvm_params = lsvm_cv.fit(X_train, y_train).best_params_\nsvm_model = LinearSVC(class_weight=best_lsvm_params[\"class_weight\"],\n                      penalty=best_lsvm_params[\"penalty\"],\n                      loss=best_lsvm_params[\"loss\"],\n                      dual=best_lsvm_params[\"dual\"],\n                      C=best_lsvm_params[\"C\"],\n                      random_state=0)\nsvm_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79a08eb7bcf5b61f2024537b3d02352d28c7b550"},"cell_type":"markdown","source":"### Model Testing & Analysis"},{"metadata":{"_uuid":"55008c38fd8d9bbcbc4caf22b7376e84338ac106"},"cell_type":"markdown","source":"Let's now see the accuracy scores achieved by these two models on the test set."},{"metadata":{"trusted":true,"_uuid":"02e025e24a60db604dab537828f612a4e18d2e26"},"cell_type":"code","source":"print(f\"The test accuracy score of the LR model is {lr_model.score(X_test, y_test)}.\")\nprint(\"The test accuracy score of the linear SVM model is \"\n      f\"{svm_model.score(X_test, y_test)}.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"256a65409416d62731f996ab1d25c96a30cc1560"},"cell_type":"markdown","source":"Though both models identify a cancerous breast mass with a high accuracy, both still make a few errors. When selecting a machine learning model to use, though we'd of course prefer having no errors, it's important to consider what types of errors our model is making. In this context, false positive errors would correspond to incorrectly diagnosing a benign tumor as malignant, while false negative errors would correspond to misdiagnosing a malignant tumor as benign. I think clearly we would much rather have false positive errors as opposed to false negative errors, since we don't want any cancerous tumors to get past our machine learning model. Recall is the standard metric used to evaluate the false negatives made by a machine learning model, but I personally tend to avoid it since a machine learning model that simply classified all labels as positive would achieve perfect recall. I will therefore use the F2-Score instead, the weighted harmonic mean of recall and precision, which lends more weight to recall without ignoring a model's precision."},{"metadata":{"trusted":true,"_uuid":"8cbe7ab6cc6c53bd187ff09e867827ae0c7f2883"},"cell_type":"code","source":"lr_preds = lr_model.predict(X_test)\nsvm_preds = svm_model.predict(X_test)\nprint(\"The test F2-Score of the LR model is \"\n      f\"{fbeta_score(y_test, lr_preds, beta=2, pos_label='M')}.\")\nprint(\"The test F2-Score of the linear SVM model is \"\n      f\"{fbeta_score(y_test, svm_preds, beta=2, pos_label='M')}.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83a130b935acca13bd7505901ca420b57310681a"},"cell_type":"markdown","source":"As we can see, the logistic regression model again performs better than the linear support vector machine, and will therefore be selected as the machine learning model to use as a breast cancer diagnoser in this work.\n\nTo gain a better understanding of the types of predictions made by this logistic regression model on the test set, let's view a confusion matrix of it's predictions."},{"metadata":{"trusted":true,"_uuid":"4fcc8abcbb24a1d9ba04e640b9d8db157c73360d"},"cell_type":"code","source":"confusion = pd.DataFrame(confusion_matrix(y_test, lr_preds))\nconfusion = confusion.div(confusion.sum().sum())\nconfusion.columns = [\"Predicted Negative\", \"Predicted Positive\"]\nconfusion.index = [\"Actual Negative\", \"Actual Positive\"]\nax = sns.heatmap(confusion, vmin=0, vmax=1, annot=True, fmt=\".0%\")\nax.set_yticklabels(ax.get_yticklabels(), rotation=0)\nax.collections[0].colorbar.set_ticks((0, .25, .5, .75, 1))\nax.collections[0].colorbar.set_ticklabels((\"0%\", \"25%\", \"50%\", \"75%\", \"100%\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83be45960f354a3666bd7c2cbccc4db1e217ce93"},"cell_type":"markdown","source":"Viewing the above confusion matrix, we can see that of all diagnoses made by the logistic regression model, around 97% of them are correct, while around 1% of predictions are false positives and 2% are false negatives.\n\nTo gain a better understanding of which breast mass features are the most significant, let's visualize the permutation importance of the features in our dataset."},{"metadata":{"trusted":true,"_uuid":"824717a5b8d2a598abca242494110e3235f232c9"},"cell_type":"code","source":"permutations = PermutationImportance(lr_model, random_state=0).fit(X_test, y_test)\neli5.show_weights(permutations, top=None, feature_names=X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c3752cf1c1a25bcef9582e3e4e79f8768b88e8a"},"cell_type":"markdown","source":"As we can see, the worst texture of the digitized cell nuclei image of a breast mass seems to be the most significant feature, while the standard error of the image concavity seems to be the least significant.\n\nLet's build partial dependence plots of the 5 most significant features to view their impact on breast tumor malignancy."},{"metadata":{"trusted":true,"_uuid":"d6c06c2577d4d4cdc66734de1e91699df061c50a","scrolled":false},"cell_type":"code","source":"feature_names=X_test.columns\ntop_features = (\"texture_worst\", \"radius_se\", \"perimeter_se\",\n                \"area_se\", \"compactness_se\")\nfor i, feature in enumerate(top_features):\n    pdp_feature = pdp.pdp_isolate(lr_model, X_test, feature_names, feature)\n    pdp.pdp_plot(pdp_feature, feature)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98748691021c0775f288a8276ffc6d3006f6bd0f"},"cell_type":"markdown","source":"As we can see viewing the above partial dependence plots, the probability of breast tumor malignancy steadily increases as the worst texture, or radius, perimeter and area standard errors respectively increase, while the probability decreases as the compactness standard error goes up for the digitized cell nuclei image of a breast mass."},{"metadata":{"trusted":true,"_uuid":"3e886d2335d61f9e3f7669c3e4dad38911da8820"},"cell_type":"markdown","source":"## Final Remarks"},{"metadata":{"trusted":true,"_uuid":"953068cd461a297a78e59a427f981d2ce508c1e6"},"cell_type":"markdown","source":"Training a logistic regression model on a dataset of digitized cell nuclei images of breast masses, we were able to build a breast cancer diagnoser with a test accuracy of about 97% and a test F2-Score of about 96%. Using this logistic regression model, we were able to find that the worst texture, and the standard error of each of the radius, perimeter, area, and compactness of these images were the best individual indicators of breast tumor malignancy. Other features such as the worst smoothness and standard error of the concavity proved to be the worst. Despite the high accuracy and F2-Score achieved by this machine learning model, further improvements can certainly be made to further reduce the errors, most importantly the false negatives, made by this model. Using the above and further feature analysis, one could remove unnecessary features and/or further engineer these features to perhaps improve upon these results."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}